[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regressions with R",
    "section": "",
    "text": "Welcome!\nWelcome to the course materials for the Regressions with R short course.\nThis course provides a comprehensive understanding of regression analysis, including the theory behind these models, their application in R, validation techniques, and the interpretation of results. The course begins with an introduction to linear regression models, before advancing to the more flexible family of generalised linear models.\nTopics covered as part of this course include:",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Regressions with R",
    "section": "How to use this book",
    "text": "How to use this book\nThis book provides a combination of written explanations, code examples, and practical exercises to allow you to practice what you have learned.\nCode examples will be provided in code blocks, such as this one:\n\n1 + 1\n\nCode in these blocks can be copied and pasted into your R session to save time when coding. We recommend typing the code yourself to familiarise yourself with the coding process and use the copy option if you are really stuck!\nThroughout the book, you will see colour-coded boxes which are used to highlight important points, give warnings, or give tips such as keyboard shortcuts.\n\n\n\n\n\n\nNote\n\n\n\nThese boxes will be used to highlight important messages, supplementing the main text.\n\n\n\n\n\n\n\n\nHint\n\n\n\nThese boxes will contain useful hints, such as keyboard shortcuts, that can make your coding life a little easier!\n\n\n\n\n\n\n\nStyle tip\n\n\nThese boxes contain style tips to ensure that your code follows the Tidyverse style guide, making it as consistent and readable as possible.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThese boxes will contain warnings and highlight areas where you need to be more cautious in your coding or analysis.\n\n\nTo make these notes as accessible as possible, they are available to view in dark mode by toggling the  button. They are also available to download as a PDF file using the  button.\nAll exercise solutions are available in the appendices. Please attempt the exercises yourself first, making full use of R’s built in help files, cheatsheets (where available), and example R code in this book. Going straight to the solutions to copy and paste the code without thinking will not help you after the course!\nSome exercises contain expandable hints, such as functions required to complete them, that can be viewed when needed. For example:\n:::{callout-caution collapse=“true} ## Exercise hint\nThe functions you will need for this exercise are filter and count. :::",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#data-used-in-the-course",
    "href": "index.html#data-used-in-the-course",
    "title": "Regressions with R",
    "section": "Data used in the course",
    "text": "Data used in the course\nThe examples and exercises in these materials are based on real world data….\nData for this course can be downloaded from the data folder of this course’s repository.\nFor more information about this data, including variable descriptions and sources, see the appendix.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#feedback-and-issues",
    "href": "index.html#feedback-and-issues",
    "title": "Regressions with R",
    "section": "Feedback and issues",
    "text": "Feedback and issues\nThis book is a work in progress and will be updated based on course feedback and requirements of participants. If you spot a bug or mistake in these notes, please let me know by raising an issue.\nIf you enjoyed using these resources and would like to find more of them or attend a live course, please visit my website, follow me on Twitter and LinkedIn, or support more free resources by buying me a coffee!.\nIf you are interested in organising a bespoke course or have consultancy opportunities you think I would be a good fit for, please get in touch!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Regressions with R",
    "section": "License",
    "text": "License\nI believe that science should not be behind a paywall, that is why these materials are available for free online, licensed under a CC BY-SA licence.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is a model?\nModelling is a process that is carried out across many different fields for a wide variety of reasons. Models aim to explain complex processes in as simple terms as possible. The goal of modelling may be to make predictions based on observed values, or to gain insights into the process, while accounting for multiple pieces of data.\nIn statistics, regression models aim to quantify the relationship between an outcome and one or more explanatory variables using a mathematical equation. They are a powerful and widely used tool that can allow us to make inferences about these underlying relationships whilst accounting for background factors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#which-type-of-regression-should-i-use",
    "href": "01_intro.html#which-type-of-regression-should-i-use",
    "title": "1  Introduction",
    "section": "1.2 Which type of regression should I use?",
    "text": "1.2 Which type of regression should I use?\nThis course will focus mostly on linear models: models with a single continuous outcome variable that assume the process can be described using a linear equation.\n\n\n\n\n\n\nNote\n\n\n\nThis does not mean that the relationships between variables must be linear. We will see later in the course how models can be extended to account for nonlinear relationships.\n\n\nWe will use linear regression models to address a research question with real-world data. Through the course, you will learn how to fit linear regression models, interpret their outcomes, ways in which models can be extended and improved, how to check models are valid, and finally how to answer the initial research question using regression.\nLater in the course, we will see how these linear models can be generalised to outcome variables beyond continuous measures, and how these model interpretations differ. Finally, we will end with a discussion about alternative models that are available beyond those covered in the course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#notes-on-r-coding-style",
    "href": "01_intro.html#notes-on-r-coding-style",
    "title": "1  Introduction",
    "section": "1.3 Notes on R coding style",
    "text": "1.3 Notes on R coding style\nTo ensure that this course is as useful as possible to those attending, all theory will be supplemented with worked example using R software. If you have never used R before, please refer to these introductory notes for an introduction to the software.\nThere are many approaches to coding within R. In this course, we will be using the tidyverse approach as I have found this to be more accessible for people that are not from a coding background. This approach requires the tidyverse suite of packages to be installed and loaded into the current R session.\nIf you have never downloaded tidyverse before, or have not used it in a long time and would like to download the latest version, run the following code in your R console:\n\ninstall.packages(\"tidyverse\")\n\n\n\n\n\n\n\nWarning\n\n\n\ntidyverse is a collection of 9 packages with many dependencies that must also be downloaded. If this is your first time installing tidyverse it may take a long time.\nDo not panic at the amount of text in your console\nThe only messages you need to worry about are any error messages which inform you of any issues in the installation process.\n\n\nOnce tidyverse is installed on your machine, use the following to load it into the current session of R:\n\nlibrary(tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\nThe command install.packages is only required the first time loading a new package or following any substantial updates. The library command must be run every time you start an R session. To save potential issues arising from unloaded packages, put any library commands at the beginning of any script file.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_pre_regression.html",
    "href": "02_pre_regression.html",
    "title": "2  Preparing for linear regression",
    "section": "",
    "text": "2.1 Why regression?\nAs with any other type of statistical analysis, we must always keep in mind the reason for carrying it out. Research questions are an often overlooked but fundamental part of any analysis plan, and should be fully defined before we carry out any analysis, or even collect any data!\nThroughout most of this course, we will be trying to answer questions about penguins in the Palmer Archipelago, Antarctica. Our research question for this course will be:\nIs body mass of penguins in the Palmer Archipelago related to their flipper size?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preparing for linear regression</span>"
    ]
  },
  {
    "objectID": "02_pre_regression.html#why-regression",
    "href": "02_pre_regression.html#why-regression",
    "title": "2  Preparing for linear regression",
    "section": "",
    "text": "Note\n\n\n\nResearch questions should be clear, concise and answerable! For a more detailed introduction to research question generation, including the PICO approach and worked examples, check out these notes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preparing for linear regression</span>"
    ]
  },
  {
    "objectID": "02_pre_regression.html#loading-the-data",
    "href": "02_pre_regression.html#loading-the-data",
    "title": "2  Preparing for linear regression",
    "section": "2.2 Loading the data",
    "text": "2.2 Loading the data\nFrom our research question, we know that we require data about penguins in the Palmer Archipelago in Antarctica, and that this data must contain information about their body mass and flipper size. This data can be loaded into R using the palmerpenguins package. More information about the data and its collection can be found on the package website or the original publication.\n\n\n\nArtwork by @allison_horst\n\n\nTo load in this data, we must first load the palmerpenguin package. If this is your first time using this package (or you have not used it in a very long time), you will need to download this from the online CRAN repository of R packages using the following code:\n\ninstall.packages(\"palmerpenguin\")\n\n\n\n\n\n\n\nWarning\n\n\n\nThe install.packages command requires an internet connection and, for some larger packages, can take some time.\nThe good news is that package installation is only required the first time a package is used on a machine or to update the package version following a substantial change.\n\n\nIf you have already used the palmerpenguin package or have downloaded it in the past, you can load the package and data into our current session of R using the following:\n\n1library(palmerpenguins)\n\n2data(penguins)\n\n\n1\n\nLoad the package into the current session of R.\n\n2\n\nLoad the penguin data from this package to our environment.\n\n\n\n\nWhen loading any data into R, we must run some checks to ensure it has been read in correctly. This includes checking all variables we expect are present, variable names are in a tidy format, and that variables have been recognised as the correct type.\n\n\n\n\n\nStyle tip\n\n\nVariable names should contain only lower case letters, numbers and underscores _. They should be clear and descriptive. If you are reading data from a particularly messy source, the janitor R package contains the clean_names function that converts existing variable names into a ‘tidy’ alternative.\n\n\n\n1View(penguins)\n\n2names(penguins)\n\n3str(penguins)\n\n\n1\n\nPreview the dataset in RStudio.\n\n2\n\nReturn variable names.\n\n3\n\nDisplay the structure of the data, including the object type, variable types, and a preview of each variable.\n\n\n\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nThe penguins dataset contains observations made on 344 penguins. There are 8 variables in the data, including body mass and flipper length, which would need to be included in our final model to answer our research question.\nThe data consists of a mixture of numeric, binary (sex) and nominal (species, island) variables which appear to be correctly specified within R.\n\n\n\n\n\n\nNote\n\n\n\nIf the data contains ordered categorical variables, ensure they are recognised as factor with the correct order assigned. If this is not the case by default, correct this before proceeding, using the mutate function to add the converted variable and the factor function with levels defined in the correct order.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preparing for linear regression</span>"
    ]
  },
  {
    "objectID": "02_pre_regression.html#exploring-the-data",
    "href": "02_pre_regression.html#exploring-the-data",
    "title": "2  Preparing for linear regression",
    "section": "2.3 Exploring the data",
    "text": "2.3 Exploring the data\nWhen we are sure that the data have been read in correctly and tidied into a useable format, we can begin to explore the data. Data exploration can include\n\nData visualisations, used to identify potential outliers, check variable distributions, etc.\nSummarising variables in the sample, to quantify aspects of the variables such as the center and spread (for numeric variables) or the distribution of observations between groups (for categorical variables)\nQuantifying bivariate relationships and differences between groups, using values such as abolute or relative differences, and correlation coefficients\n\nAlthough data exploration will not allow us to answer the research question, it is a necessary step in the analysis process to build the best possible model. It allows us to identify potential issues that may arise before we encounter them.\n\n2.3.1 Data visualisation\nData visualisation can be an effective method of exploring the data and generating hypotheses. In our example, we are interested in understanding the relationship between penguin’s body mass and flipper size. Therefore, it makes sense to begin by visualising these variables. As both variables are continuous, we can use a histogram to visualise them:\n\n\n\n\n\n\nWarning\n\n\n\nFrom this point on, we will be using the ggplot2 package which is part of tidyverse to generate visualisations. Make sure you have loaded the tidyverse package to your current session of R using code from the previous section.\n\n\nggplot(data = penguins) +\n  geom_histogram(aes(x = body_mass_g), \n                 colour = \"black\", fill = \"grey45\") +\n1  labs(x = \"body mass (g)\") +\n2  theme_light(base_size = 12)\n\nggplot(data = penguins) +\n  geom_histogram(aes(x = flipper_length_mm), \n                 colour = \"black\", fill = \"grey45\") +\n  labs(x = \"flipper length (mm)\") + \n  theme_light(base_size = 12)  \n\n\n1\n\nAdd a tidier label to the x-axis\n\n2\n\nChange the default theme and ensure text is at least 12pt in size.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Body mass\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Flipper length\n\n\n\n\n\n\nThese histograms show that neither variable have any outliers of concern. The flipper length variable follows a bi-modal distribution, suggesting that there may be groupings in the data that may be important to explain differences in the sample. The outcome variable, body mass, follows a slightly positively skewed distribution.\n\n\n\n\n\n\nNote\n\n\n\nThere is no requirement that our outcome must follow a normal distribution. A normal distribution is a naturally occurring distribution in many settings, this is why we often use this as a comparison.\n\n\nWe may also want to visualise the relationship between body mass and flipper length in our sample to generate a hypothesis regarding the answer to our research question. A scatterplot is an appropriate visualisation to investigate the relationship between two numeric variables:\n\nggplot(data = penguins) +\n1  geom_point(aes(y = body_mass_g, x = flipper_length_mm)) +\n  labs(y = \"body mass (g)\", x = \"flipper length (mm)\") +\n  theme_light(base_size = 12)\n\n\n1\n\nThe outcome variable should be displayed on the y-axis, the explanatory variable on the x-axis.\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Scatterplot showing the relationship between penguins’ body mass and flipper length\n\n\n\n\n\nThe scatterplot shows a strong, positive, linear relationship between body mass and flipper length: as flipper length increases, body mass tended to also increase.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preparing for linear regression</span>"
    ]
  },
  {
    "objectID": "02_pre_regression.html#exercise-1",
    "href": "02_pre_regression.html#exercise-1",
    "title": "2  Preparing for linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nUsing appropriate visualisations, investigate whether there are other variables that may explain differences in body mass. Consider whether any of these variables may be confounding the relationship between body mass and flipper length, and whether they should be included in the model.\n\n\n\n\n\n\nHint\n\n\n\n\n\nConsider changing the colour of points in Figure 2.3 to investigate whether the relationship between body mass and flipper length differs between species or sex.\nReplace flipper length with other continuous variables to consider whether they may also contribute to differences in body mass.\nUse facet_wrap to create plots facetted by categorical variables in the data to compare relationships without overlapping points.\nIf you are REALLY stuck, an example solution can be found here.\n\n\n\n\n2.3.2 Summarising trends\nSummary statistics are useful for quantifying different aspects of a sample. As they relate only to the sample, they cannot make inferences about a target population, nor can they answer our research question. They can be used in data exploration though to quantify trends between variables and differences between categories to generate hypotheses about how we may answer our research question.\nWe saw in Figure 2.3 that there was a strong, linear relationship between penguins’ body mass and flipper length. This relationship can be quantified using Pearson’s correlation coefficient, a measure of linear association between numeric variables.\n\n\n\n\n\n\nNote\n\n\n\nCorrelation coefficients take values between -1 and 1, with 0 representing no association and positive/negative results representing positive/negative associations. The closer the result is to $$1, the stronger an association is.\n\n\n\ncor(penguins$body_mass_g, penguins$flipper_length_mm,\n1    use = \"complete.obs\")\n\n\n1\n\nAs there are missing values in the penguins dataset, we must specify the function use only complete observations to avoid an NA result.\n\n\n\n\n[1] 0.8712018\n\n\nAs expected, there is a very strong, positive association between body mass and flipper length. Correlation coefficients can be presented with p-values to make inferences on a target population. However, they provide very little information about the nature of the relationship between variables, for example the magnitude of the relationship.\nThat is where linear regression comes in!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preparing for linear regression</span>"
    ]
  },
  {
    "objectID": "03_linear_reg.html",
    "href": "03_linear_reg.html",
    "title": "3  Linear regression",
    "section": "",
    "text": "3.1 Simple linear regression\nSimple linear regression refers to a model with a single continuous outcome and a single explanatory variable. The regression model will assume the relationship between the outcome \\(Y\\) and explanatory variable \\(X\\) can be described using the equation:\nWhere \\(X\\) is also a continuous variable, this represents the equation of a straight line, where \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the gradient. The results of this linear regression model will provide the equation of the line of best fit.\nTo fit a simple linear regression to our data in R, we use the lm function and return the model results using summary:\n1lm_flipper &lt;- lm(body_mass_g ~ flipper_length_mm,\n                 data = penguins)\n\n2summary(lm_flipper)\n\n\n1\n\nFirst, we define the regression model equation.\n\n2\n\nSummary provides model output, including coefficient estimates, p-values, and other model summaries.\n\n\n\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1058.80  -259.27   -26.88   247.33  1288.69 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5780.831    305.815  -18.90   &lt;2e-16 ***\nflipper_length_mm    49.686      1.518   32.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.3 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.759, Adjusted R-squared:  0.7583 \nF-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16\nThere is a lot of information given here, a quick summary of the important points (for now) are:\nUsing the model output gives us the equation of the line of best fit:\nThis line can be added to the scatterplot to visualise the results:\nggplot(data = penguins) +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_abline(intercept = coefficients(lm_flipper)[1], \n              slope = coefficients(lm_flipper)[2], \n              colour = \"red\", linewidth = 2) +\n  labs(x = \"flipper length (mm)\", y = \"body mass (g)\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\nFigure 3.1: Scatterplot of body mass and flipper length with a line of best fit added\nTo obtain confidence intervals for the coefficient estimates, use the confint function:\n1confint(lm_flipper)\n\n\n1\n\nBy default, this returns the 95% confidence interval. This can be adjusted using the level argument. For example, level = 0.9 would return the 90% confidence interval.\n\n\n\n\n                        2.5 %      97.5 %\n(Intercept)       -6382.35801 -5179.30471\nflipper_length_mm    46.69892    52.67221\nThe 95% confidence interval for the flipper length coefficient it [46.7, 52.67]. Therefore, we are 95% confident that for every millimeter longer a penguin’s flipper is, we would expect their body mass to be between 46.7 and 52.67 higher.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03_linear_reg.html#simple-linear-regression",
    "href": "03_linear_reg.html#simple-linear-regression",
    "title": "3  Linear regression",
    "section": "",
    "text": "\\[\nY = \\beta_0 + \\beta_1x_1\n\\tag{3.1}\\]\n\n\n\n\n\n\n(Intercept): this returns the estimate of \\(\\beta_0\\) from Equation 3.1, the expected value of the outcome where all covariates are 0.\n\n\n\n\n\n\n\nNote\n\n\n\nOften, the intercept value will not have a meaningful interpretation. For example, here it tells us the expected body mass of penguins with flipper length 0mm is -5780.83g.\n\n\n\nflipper_length_mm: this returns the estimate of \\(\\beta_1\\) from Equation 3.1, the gradient. This can interpreted as the expected change in the outcome for every unit increase of the associated covariate. In this example, penguins’ body mass is expected to be 49.69g higher for every millimeter longer their flippers were.\n\n\n\n\n\n\n\nNote\n\n\n\nAs the linear equation assumes an additive relationship between the outcome nad covariate, we can use this to make estimates about differences in the outcome based on the difference in covariate. For example, if there were two penguins and one had flippers that were 1cm (10mm) longer, we would expect their body mass to be (49.69 \\(\\times\\) 10 =) 496.9 heavier.\n\n\n\nPr(&gt;|t|): the p-value associated with each coefficient estimate, testing the null hypothesis of no association (\\(\\beta_i = 0\\)). In this model, the p-value associated with \\(\\beta_1\\) is so small that it cannot be written in its entirety. Therefore we can state that there was a statistically significant association between flipper length and body mass.\n\n\n\n\n\n\n\nNote\n\n\n\n&lt;2e-16 is scientific notation for &lt; 0.0000000000000002.\n\n\n\nMultiple R-squared: the \\(R^2\\) value represents the proportion of variance in the outcome variable explained by the model. In this case, the proportion is 0.759 or, if we convert it into a percentage, (0.759 \\(\\times\\) 100 =) 75.9% of the variation in body mass has been explained by flipper length. The p-value under this estimate relates to the R-squared value and tests the null hypothesis that R-squared = 0 (i.e. the model does not explain any of the outcome). In this case, the p-value is too small to be printed (given as &lt;2e-16), indicating the model explains a significant amount of the variation in body mass.\n\n\n\n\n\n\n\nNote\n\n\n\nWhere there is a single continuous explanatory variable, the \\(R^2\\) value is the Pearson correlation squared. If we take the square root of this value, it will give us the same as the correlation value estimated earlier:\n\nr_sq &lt;- summary(lm_flipper)$r.squared\n\ncor(penguins$body_mass_g, penguins$flipper_length_mm,\n    use = \"complete.obs\")\n\n[1] 0.8712018\n\nsqrt(r_sq)\n\n[1] 0.8712018\n\n\n\n\n\n\nbody mass = -5780.83 + 49.69 \\(\\times\\) flipper length\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe cannot make any causal statements about increases in flipper length causing increases in body mass as there may be underlying factors confounding these results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03_linear_reg.html#multiple-linear-regression",
    "href": "03_linear_reg.html#multiple-linear-regression",
    "title": "3  Linear regression",
    "section": "3.2 Multiple linear regression",
    "text": "3.2 Multiple linear regression\nMultiple, or multivariable, linear regression is a powerful extension which allows models to take account of other observed variables. This is important as confounding variables can cause misleading results where they mask or even create spurious associations between variables.\nAlthough the previous model appears to explain a large proportion of the variation in body mass, we want to ensure that this association is not influenced by other variables. For example, we may wish to account for differences in species which is likely to be associated with body mass:\n\nggplot(data = penguins) +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g,\n                 colour = species)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  labs(x = \"flipper length (mm)\", y = \"body mass (g)\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\nFigure 3.2: Scatterplot investigating the relationship between body mass, flipper length, and species in penguins.\n\n\n\n\n\nThe scatterplot clearly shows that species is highly associated with both body mass and flipper length, making it a potential confounding variable, and therefore a variable we should consider including in our model. As species is a categorical variable, we must include it into the model as dummy variables.\n\n3.2.1 Dummy variables\nDummy variables take the value 1 or 0, with 1 representing inclusion in a group. Categorical variables require one less dummy variable than the number of categories the variable represents to be included in a regression model. For example, binary variables require one dummy variable, the species variable requires two. The category which does not have an associated dummy variable is implicitly included in the model as the reference group.\nFor example, the species variable would be converted from a single nominal variable to two dummy variables:\n\n\n\n\n\nspecies\nchinstrap\ngentoo\n\n\n\n\nAdelie\n0\n0\n\n\nAdelie\n0\n0\n\n\nAdelie\n0\n0\n\n\nGentoo\n0\n1\n\n\nGentoo\n0\n1\n\n\nAdelie\n0\n0\n\n\nAdelie\n0\n0\n\n\nAdelie\n0\n0\n\n\nGentoo\n0\n1\n\n\nAdelie\n0\n0\n\n\n\n\n\nR converts factor variables into dummy variables automatically when they are included in the model formula. It is important to check that variables are classified as factor before adding them to the model as otherwise, they will be treated as numbers.\n\nclass(penguins$species)\n\n[1] \"factor\"\n\nlevels(penguins$species)\n\n[1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"   \n\n\nThe class function returns the type of variable and levels lists the levels of factor variables in the order that they have been specified. R uses the first level of a factor variable as the reference group in a linear model. To change this order, we can use the fct_relevel function from tidyverse’s forcats package. For example, if we want to set the Gentoo species as the reference group, we would use the following:\n\npenguins_new &lt;- mutate(penguins,\n                       species_gentoo = fct_relevel(species, \"Gentoo\"))\n\nlevels(penguins_new$species_gentoo)\n\n[1] \"Gentoo\"    \"Adelie\"    \"Chinstrap\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe choice of reference group depends on the data being analysed. If there is a clear reference, for example some control group that we would like to compare all others to, this should be set as the first level. If there is no clear choice of reference, sometimes people choose the largest group.\n\n\nThe factor is then added into the model formula within the lm function:\n\nlm_flipper_spec &lt;- lm(body_mass_g ~ flipper_length_mm + species,\n                      data = penguins)\n\nsummary(lm_flipper_spec)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-927.70 -254.82  -23.92  241.16 1191.68 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -4031.477    584.151  -6.901 2.55e-11 ***\nflipper_length_mm    40.705      3.071  13.255  &lt; 2e-16 ***\nspeciesChinstrap   -206.510     57.731  -3.577 0.000398 ***\nspeciesGentoo       266.810     95.264   2.801 0.005392 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 375.5 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7826,    Adjusted R-squared:  0.7807 \nF-statistic: 405.7 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nThe output contains two additional coefficient estimates (one for each of the dummy variables). The updated linear equation estimated from this model is as follows:\nbody mass = -4031.48 + 40.71 \\(\\times\\) flipper length + -206.51 \\(\\times\\) chinstrap + 266.81 \\(\\times\\) gentoo\nNotice that the coefficient estimate for flipper length has changed. This is because coefficient estimates in multiple regression models give the estimated change in the outcome per unit increase in the associated covariate after adjusting for everything else in the model. Therefore, body mass is expected to increase by 40.71 for every 1mm increase in flipper length after adjusting for species differences.\nCoefficients associated with dummy variables give the average difference between that group and the reference group, assuming all other variables are equal. For example, Gentoo penguins with the same flipper length as an Adelie penguin were expected to weigh 266.81 more on average.\nIf we were to visualise this, dummy variables add parallel lines of best fit, one for each group:\n\nggplot(data = penguins) +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n1  geom_abline(intercept = coefficients(lm_flipper_spec)[1],\n              slope = coefficients(lm_flipper_spec)[2], \n              colour = \"#1B9E77\", linewidth = 2) +\n2  geom_abline(intercept = (coefficients(lm_flipper_spec)[1] +\n                        coefficients(lm_flipper_spec)[3]), \n              slope = coefficients(lm_flipper_spec)[2], \n              colour = \"#D95F02\", linewidth = 2) +\n3  geom_abline(intercept = (coefficients(lm_flipper_spec)[1] +\n                        coefficients(lm_flipper_spec)[4]), \n              slope = coefficients(lm_flipper_spec)[2], \n              colour = \"#7570B3\", linewidth = 2) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  labs(x = \"flipper length (mm)\", y = \"body mass (g)\") +\n  theme_light(base_size = 12)\n\n\n1\n\nThe equation of the line for Adelie penguins will be  body mass = -4031.48 + 40.71 \\(\\times\\) flipper length + -206.51 \\(\\times\\) 0 + 266.81 \\(\\times\\) 0  = -4031.48 + 40.71 \\(\\times\\) flipper length\n\n2\n\nThe equation of the line for Chinstrap penguins will be  body mass = -4031.48 + 40.71 \\(\\times\\) flipper length + -206.51 \\(\\times\\) 1 + 266.81 \\(\\times\\) 0  = (-4031.48 + -206.51) + 40.71 \\(\\times\\) flipper length\n\n3\n\nThe equation of the line for Gentoo penguins will be  body mass = -4031.48 + 40.71 \\(\\times\\) flipper length + -206.51 \\(\\times\\) 0 + 266.81 \\(\\times\\) 1  = (-4031.48 + 266.81) + 40.71 \\(\\times\\) flipper length\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Scatterplot of body mass, flipper length and species, with lines of best fit added\n\n\n\n\n\nAll three coefficients had very low p-values and are therefore significantly different from 0, or no association. We can obtain the confidence intervals for the coefficient estimates using confint:\n\nconfint(lm_flipper_spec)\n\n                        2.5 %      97.5 %\n(Intercept)       -5180.50685 -2882.44693\nflipper_length_mm    34.66468    46.74612\nspeciesChinstrap   -320.06672   -92.95352\nspeciesGentoo        79.42513   454.19408\n\n\nNotice that none of these confidence intervals contain 0, supporting the p-values that we are not compatible with no association between these variables and body mass at the target population level.\n\n\n\n\n\n\nWarning\n\n\n\nWhen including categorical variables, we must include all associated dummy variables or none. This is the case even when some coefficients are considered significant and some are not.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03_linear_reg.html#model-comparisons",
    "href": "03_linear_reg.html#model-comparisons",
    "title": "3  Linear regression",
    "section": "3.3 Model comparisons",
    "text": "3.3 Model comparisons\nWhen choosing the best possible model to address a research question, the aim is usually to find the most parsimonious model for the job. That means the simplest model to explain as much as possible. Model choice should first and foremost be driven by the motivation for the analysis (the research question), our prior knowledge of what other factors are important, and common sense based on preliminary checks.\nModel choice should not be determined solely by p-values.\nThere are a number of tools available to help us select the most parsimonious model but these should be considered after motivation, prior knowledge and common sense.\n\n3.3.1 Adjusted R-squared value\nThe Multiple R-squared value provided by the summary function provides a measure of the proportion of the variation of the outcome explained by the model. As we add covariates to the model this value will increase, even by a tiny amount, regardless of whether the model addition is ‘worthwhile’. That is why the Adjusted R-squared value is also provided.\nThe Adjusted R-squared penalises the R-squared value based on the complexity of the model: the more complex a model is, the higher the penalty. Although the Multiple R-squared increases with every model addition, the Adjusted R-squared will only increase where the model is considered improved. Therefore, the adjusted value can be used to compare between models to identify the most parsimonious.\nWe can compare the two models fitted in this section using the Adjusted R-squared to identify the most parsimonious. The higher the Adjusted R-squared, the better the fit:\n\nsummary(lm_flipper)$adj.r.squared\n\n[1] 0.7582837\n\nsummary(lm_flipper_spec)$adj.r.squared\n\n[1] 0.7807187\n\n\nThe model including species had a higher Adjusted R-squared and is therefore considered the most parsimonious in this case.\n\n\n\n\n\n\nNote\n\n\n\nThe Adjusted R-squared value should be used as a model comparison to identify the most parsimonious model. When the best possible model has been chosen, the results should be presented with the Multiple R-squared as a summary.\n\n\n\n\n3.3.2 Information criterion\nAnother method to assess model goodness of fit is the deviance (also known as -2 log-likelihood). This is a measure of how much a model deviates from a hypothetical full model that predicts each point perfectly. This full model would not be useful to make inferences from as it would only describe the sample it is based on but the deviance can compare similar models to help find the most parsimonious.\nThe deviance alone is not useful as there is no value where a deviance is ‘small enough’ to represent a good fit. However, the deviance can be transformed into a score known as an information criterion. These provide a measure of how parsimonious models are by penalising their deviance based on the number of variables included. If the information criterion is lower after adding extra variables, this means the extra complexity explains enough to be worthy of inclusion.\nThere are a number of information criterions available with different penalties. Two of the most common are the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). These scores will usually give similar results but may differ slightly as they attach different penalties (the BIC usually prefers simpler models to the AIC).\n\nAIC(lm_flipper)\n\n[1] 5062.855\n\nAIC(lm_flipper_spec)\n\n[1] 5031.523\n\nBIC(lm_flipper)\n\n[1] 5074.359\n\nBIC(lm_flipper_spec)\n\n[1] 5050.697\n\n\nThe model including species had the lowest value for both information criterion, indicating that the addition of species has improved the model enough to consider it worthwhile.\n\n\n3.3.3 Prediction metrics\nRoot mean squared error (RMSE) and mean absolute error (MAE) are model comparison metrics that compare model predictions to the observed values. The smaller the RMSE or MAE, the better the model is at predicting the outcome. There are other similar metrics that can be used on place of these, however the RMSE and MAE are useful as they give the result on the same scale as the outcome.\nAs the name suggests, the RMSE is estimated by finding the root mean squared error (difference between the observed outcome, \\(y_i\\) and the predicted outcome from the model, \\(\\hat{y}_i\\):\n\\(\\sqrt{\\frac{1}{n}\\sum{(y_i - \\hat{y}_i)^2}}\\)\nThe MAE returns the mean absolute error between the observed and predicted outcome:\n\\(\\frac{1}{n}\\sum{|y_i - \\hat{y}_i|}\\)\nBoth these metrics can be estimated using the Metrics package in R, inputting the observed outcome (here, body_mass_g) and the predicted outcome from the model (obtained using the predict function):\n\nlibrary(Metrics)\n\nrmse(lm_flipper$model$body_mass_g, predict(lm_flipper))\n\n[1] 393.1236\n\nrmse(lm_flipper_spec$model$body_mass_g, predict(lm_flipper_spec))\n\n[1] 373.3325\n\nmae(lm_flipper$model$body_mass_g, predict(lm_flipper))\n\n[1] 313.0018\n\nmae(lm_flipper_spec$model$body_mass_g, predict(lm_flipper_spec))\n\n[1] 296.4359\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure you have installed the Metrics package before running the prediction metrics.\nWe have extracted the observed outcome from the model object rather than the original data. This is because there is missing data in the penguins data which is removed when we fit the models. Using the original data will lead to variables of different length and produce an error in the functions.\n\n\nBoth metrics agree that the model containing species was better at predicting the body mass of penguins than the model just containing flipper length. Usually we would just choose one of these metrics rather than displaying both. In most cases, they will agree, where they don’t it is because the RMSE is more sensitive to outliers, unlike the MAE which treats all values equally.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03_linear_reg.html#model-diagnostics",
    "href": "03_linear_reg.html#model-diagnostics",
    "title": "3  Linear regression",
    "section": "3.4 Model diagnostics",
    "text": "3.4 Model diagnostics\nBefore communicating the results of a model, we must ensure that the model we have used is valid and appropriate for the data. Linear regression is a parametric method which means there are assumptions that must be checked to ensure that the model we are using is appropriate. Linear regression assumptions can be remembered using the LINE acronym:\n\nLinearity: the relationship between the (mean) outcome and explanatory variable(s) can be described using a linear equation\nIndependence: explanatory variables must be independent of one another\nNormally distributed residuals: model error terms must follow a normal distribution\nEquivalent variance: residuals must have a constant variance across all values of the outcome (also known as heteroskedasticity)\n\n\n3.4.1 Multicollinearity\nMulticollinearity occurs when one or more of the explanatory variables can be explained by other covariates in the model. In other words, the explanatory variables are not independent of one another. When multicollinearity exists within a model, coefficient estimates become unstable and results may no longer be valid. The level of dependence between continuous covariates can be quantified using the variance inflation factor (VIF).\nThe VIF is estimated for each covariate, \\(X_i\\), by fitting a linear model where \\(X_i\\) is set as the outcome, with all other covariates included as explanatory variables. The VIF of this coefficient is estimated using the R-squared value of that model:\n\\(VIF_i = \\frac{1}{1 - R_i^2}\\)\nThe larger the VIF, the more collinearity present in a model. For example, a VIF of 10 occurs when 90% of the covariate is explained by other variables in the model (where \\(R^2\\) = 0.9). There is no agreed level above which multicollinearity causes an issue in the model, although 5 is a common cut-off above which a VIF is considered high.\n\n\n\n\n\n\nNote\n\n\n\nAlthough correlation between covariates may give some indication of potential multicollinearity, regression is able to account for some amount of correlation. Although correlated covariates can be included, model interpretations may become more complex as the coefficients are estimated after adjusting for other variables in the model, often producing unexpected results.\n\n\nWhere a model contains categorical variables, the generalised VIF (GVIF) can be used. However, as some categorical variables are included into a model with multiple dummy variables, the GVIF must be adjusted to account for differences in the degrees of freedom (df). To make GVIF comparable across variables with different degrees of freedom, we apply the correction:\n\\(GVIF^\\frac{1}{2df}\\)\nThis value is also known as the generalised standard error inflation factor (GSEIF). For numeric or binary covariates, this is the square root of the GVIF. Therefore, the rule of thumb cut-offs for problematic levels of multicollinearity when considering GSEIF will be the square root of the VIF cut-offs (\\(\\sqrt{5} = 2.236\\)).\nVIFs, GVIFs and (when the model contains categorical variables) GSEIFs are estimated in R using the car package:\n\n\n\n\n\n\nWarning\n\n\n\nIf you have never used the car R package before, ensure it is installed on your machine using the install.packages function.\n\n\n\nlibrary(car)\n\nvif(lm_flipper_spec)\n\n                      GVIF Df GVIF^(1/(2*Df))\nflipper_length_mm 4.509154  1        2.123477\nspecies           4.509154  2        1.457215\n\n\nBoth GSEIF values are below the cut-off value, indicating there is no issue with multicollinearity in this model.\n\n\n\n\n\n\nNote\n\n\n\nWhere there is evidence of multicollinearity in a model, one or more of the covariates causing the issue must be removed from the model. Otherwise, coefficient estimates will be unstable and inferential results may be invalid.\n\n\n\n\n3.4.2 Residuals\nThe other three assumptions of linear regression can be checked using residuals, or error terms. Residuals are calculated by finding the difference between the observed outcome from the data and the predicted outcome using the model. Large residuals are an indication of poor model fit and can be used to improve a model. Residuals can be obtained from a model object in R using the resid function.\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Linearity\nOne of the major assumptions underpinning linear models is that the mean outcome can be described as a linear equation of the covariates present in the model. Where there is only a single numeric variable or one numeric and one categorical variable, this assumption can be checked using a scatterplot. However, when models become more complex, we are no longer able to check this assumption graphically.\nA better approach to checking the linearity assumption of regression models is to plot a scatterplot of model residuals against each covariate. If the assumption of linearity is valid, residual points should be randomly scattered around 0 without any obvious patterns. To produce these plots, we simply extract the residuals using resid and produce a scatterplot using ggplot and geom_point:\n\npenguins_resid &lt;- lm_flipper_spec$model %&gt;%\n  mutate(residuals_flipper_spec = resid(lm_flipper_spec))\n\nggplot(data = penguins_resid) +\n  geom_point(aes(x = flipper_length_mm, y = residuals_flipper_spec)) +\n  labs(y = \"residuals\", x = \"flipper length (mm)\") +\n  geom_hline(yintercept = 0, colour = \"red\") +\n  theme_light(base_size = 12)\n\n\n3\n\nAdding a reference line where residuals = 0 can help check this assumption\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins_resid) +\n  geom_point(aes(x = species, y = residuals_flipper_spec)) +\n  labs(y = \"residuals\", x = \"species\") +\n3  geom_hline(yintercept = 0, colour = \"red\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nBoth plots show residuals randomly scattered around the reference line of 0, with no patterns in the points. This indicates that the assumption of linearity is valid in this case.\n\n\n\n\n\n\nWarning\n\n\n\nWhere the assumption of normality is not appropriate, steps must be taken to resolve this or another method must be used.\nOne way to overcome lack of linearity is to transform the covariate that does not adhere to this assumption. For example, if the relationship between the outcome and a covariate is more curved than linear, a polynomial term (\\(x^2\\)) may be considered. Be cautious when applying transformations as this will change the interpretation of model coefficients (they will no longer be on the same scale as the original data).\nIf simple transformations will not overcome a lack of linearity, or where we need coefficients to be interpretable on the data scale, we could consider generalised additive models, an alternative statistical modelling approach that can be used to model nonlinear relationships.\n\n\n\n\n3.4.4 Normally distributed residuals\nA common misconception about linear regression is that the outcome variable must be normally distributed. This is not the case, but the residuals must be. This can be easily checked using a histogram:\n\nggplot(data = penguins_resid) +\n  geom_histogram(aes(x = residuals_flipper_spec), \n                 colour = \"black\", fill = \"grey45\") +\n  labs(x = \"residuals\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nThe histogram shows residuals follow an approximately normal distribution, centred around 0.\n\n\n3.4.5 Heteroskedasticity\nThe final assumption to check when using linear regression is that the residuals have a constant variance across all observations. This can be checked using a scatterplot of the residuals against the observed outcome. We would hope to see points scattered randomly around 0. If the variance is not constant, for example if we observe a funnel shape, we must rethink our model:\n\nggplot(data = penguins_resid) +\n  geom_point(aes(x = body_mass_g, y = residuals_flipper_spec)) +\n  geom_hline(yintercept = 0, colour = \"red\") +\n  labs(x = \"body mass (g)\", y = \"residuals\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nThis plot shows evidence of heteroskedasticity: as body mass increases, the variance of the residuals also appears to increase. Therefore, this model could not be used without improvements.\n\n\n3.4.6 Influential observations\nAnother consideration when fitting regression models is the existence (and impact) of influential observations. Observations may be influential if they are outliers or behave differently to other points, which can lead them pulling the model away from the majority of the sample. This can lead to models that are not representative of the majority of the data.\n\n\n\n\n\n\nNote\n\n\n\nAs with other outliers, influential observations should not necessarily be removed from an analysis if they are part of the target population we would like to address. They may be part of an underrepresented part of the population that was not captured in the random sample.\n\n\nOne of the most common measures of influence is known as Cook’s distance. Cook’s distances provide a measure of how much the removal of each observation would change the model. The larger the Cook’s distance, the more the observation changes the model, making the point more influential.\nThere are no agreed guidelines giving a cut-off value above which an observation becomes ‘influential’ (although some have stated around 0.5). The best approach is best to plot Cook’s distances and identify extreme values by eye. The Cook’s distance can be plotted as follows:\n\nplot(lm_flipper_spec, which = 4)\n\n\n\n\n\n\n\n\nBy default, R returns the row number of observations it considers ‘influential’ which can then be used to improve the model.\n\nR uses an arbitrary method of identifying ‘influential’ observations. This means these observations are not necessarily problematic. In this case, all Cook’s distances are below 0.025, making their level of influence very low.\n\nR recognised rows 40, 170 and 315 as potentially influential. In some cases, viewing these rows can give us ideas about variables that are not in the current model but explain the differences in these observations that may improve the model:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nDream\n36.5\n18.0\n182\n3150\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.2\n14.5\n209\n4800\nfemale\n2007\n\n\nChinstrap\nDream\n49.0\n19.5\n210\n3950\nmale\n2008\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\nR contains a generic plot function that returns model diagnostic plots, including residual plots and Cook’s distance when applied to a linear model object.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03_linear_reg.html#exercise-2",
    "href": "03_linear_reg.html#exercise-2",
    "title": "3  Linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nUsing everything you have learned up to this point, use linear regression to answer the research question posed earlier:\nIs body mass of penguins in the Palmer Archipelago related to their flipper size?\n:::{callout-caution collapse=“true”} ## Exercise hint\nExplore the data to identify variables that are likely to be related to body mass that could be confounders. This includes visualising and summarising the sameple.\nAdd variables into the model, using model comparisons such as the adjusted R-squared, information criterions and RMSE/MAE to understand whether these improve the model.\nWhen you have chosen what you consider to be the best possible model, check the linear regression assumptions are met and present your answer.\nIf you are REALLY stuck, an example solution can be found here. :::",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "04_glm.html",
    "href": "04_glm.html",
    "title": "4  Generalised linear models",
    "section": "",
    "text": "4.1 Generalised linear models in R\nTo fit a GLM in R, we use the glm function. The model specification is the same as the lm function but with an optional family argument where the outcome is not continuous.\nWe can fit the linear model from the previous section using the following code:\nglm_penguin &lt;- glm(body_mass_g ~ flipper_length_mm + species,\n1                   data = penguins, family = gaussian)\n\nsummary(glm_penguin)\n\n\n1\n\nGaussian distribution is another term for normal distribution.\n\n\n\n\n\nCall:\nglm(formula = body_mass_g ~ flipper_length_mm + species, family = gaussian, \n    data = penguins)\n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -4031.477    584.151  -6.901 2.55e-11 ***\nflipper_length_mm    40.705      3.071  13.255  &lt; 2e-16 ***\nspeciesChinstrap   -206.510     57.731  -3.577 0.000398 ***\nspeciesGentoo       266.810     95.264   2.801 0.005392 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 141026.6)\n\n    Null deviance: 219307697  on 341  degrees of freedom\nResidual deviance:  47666988  on 338  degrees of freedom\n  (2 observations deleted due to missingness)\nAIC: 5031.5\n\nNumber of Fisher Scoring iterations: 2\nThe lm and glm functions will provide the same results when fitting a linear model, although some of the output provided in the summary is slightly different (lm will provide the R-squared values by default, whereas glm provides deviance and the AIC).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "04_glm.html#generalised-linear-models-in-r",
    "href": "04_glm.html#generalised-linear-models-in-r",
    "title": "4  Generalised linear models",
    "section": "",
    "text": "Note\n\n\n\nFor a full list of family options, open the ?family help file.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "04_glm.html#poisson-regression",
    "href": "04_glm.html#poisson-regression",
    "title": "4  Generalised linear models",
    "section": "4.2 Poisson regression",
    "text": "4.2 Poisson regression\nPoisson regression is used where the model outcome is either a count or rate. Linear regression would not be appropriate for count data as predictions could be negative or non-integer values. To overcome this, the model is fitted to a transformed version of the outcome. In theory, any mathematical function could be used as a link function as long as there is an opposite function that would return values to their original scale, making model results interpretable.\n\n\n\n\n\n\nNote\n\n\n\nWe do not apply any transformation to the data, this is done by the computer when we are fitting the model. The coefficients will be estimated in relation to the log-transformed outcome rather than the original variable.\n\n\n\n4.2.1 Logarithm and exponential transformations\nThe transformation applied to the outcome when fitting a poisson regression is the natural log function (also known as \\(ln\\) or \\(log_e\\) (log to the base \\(e\\)). The opposite of this transformation is the exponential function (sometimes written as \\(exp\\) or \\(e\\)).\n\n\n\n\n\n\n\n\n\nThe natural log function is useful when dealing with counts as it takes non-negative variables and ‘stretches’ them, allowing them to take any value between \\(\\pm\\infty\\). To return to the original data, we just apply the exponential function to the transformation:\n\n\n\n\n\nOriginal values (x)\nln(x)\nexp(x)\n\n\n\n\n-0.5\n-\n-\n\n\n0.0\n-\n-\n\n\n0.5\n-0.69\n0.5\n\n\n1.0\n0\n1\n\n\n2.0\n0.69\n2\n\n\n5.0\n1.61\n5\n\n\n10.0\n2.3\n10\n\n\n100.0\n4.61\n100\n\n\n1000.0\n6.91\n1000\n\n\n\n\n\nThe exponential and natural log transformations have some interesting properties that we must be aware of before interpreting poisson regression results. This is because model results may need to be transformed before they can be interpreted.\nFor example, the exponential function takes an additive relationship and converts it into a multiplicative one:\n\\(e^{a + b} = e^a \\times e^b\\)\ne.g., \\(e^5 = e^{2 + 3} = e \\times e \\times e \\times e \\times e = (e \\times e) \\times (e \\times e \\times e) = e^2 \\times e^3\\)\nMultiplicative relationships become exponential relationships:\n\\(e^{a \\times b} = (e^a)^b = (e^b)^a\\)\ne.g., \\(e^6 = e^{2 \\times 3} =  e \\times e \\times e \\times e \\times e \\times e = (e \\times e) \\times (e \\times e) \\times (e \\times e) = (e \\times e)^3 = (e^2)^3\\)\nThe natural log also has some important properties that we must be aware of during the poisson regression process:\n\\(ln(a+b)=ln(a) \\times ln(b)\\)\ne.g., \\(ln(8) = ln(6 + 2) = ln(6) \\times ln(2)\\)\n\\(ln(a − b) = ln(a) \\div ln(b)\\)\ne.g., \\(ln(5) = ln(9 − 4) = ln(9) \\div ln(4)\\)\n\n\n4.2.2 Poisson regression for count data\nTo demonstrate poisson regression, we will be using data from the cancer registry and census in the USA from 2015. This data can be downloaded from the course repository and more information about the dataset can be found in the course appendix.\nThe research question we will aim to answer using this data is:\nIs cancer mortality associated with poverty levels in the USA?\nRegardless of what model comparison statistics find, our final model must have number of cancer deaths as the outcome and some measure of poverty included as an explanatory variable.\nWe begin be exploring the bivariate relationship between these variables by loading the data and producing a scatterplot:\n\n1cancer_reg &lt;- read_csv(\"data/cancer_reg.csv\")\n\nggplot(data = cancer_reg) +\n  geom_point(aes(x = poverty, y = number_death)) +\n  labs(x = \"% of residents living in poverty\", \n       y = \"Number of cancer deaths\") +\n  theme_light(base_size = 12)\n\n\n1\n\nThis code requires the cancer_reg,csv file to be saved in a folder within your working directory named data. If the data are saved in the working directory, remove the data/ prefix.\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot does not appear to show any clear relationship between the poverty levels in US counties and the number of cancer deaths. However, this may be due to the smaller number of counties with high levels of poverty in the sample.\nWe can quantify the level of this relationship using a simple1 poisson regression model:\n\nglm_cancer_pov &lt;- glm(number_death ~ poverty, data = cancer_reg,\n                      family = poisson)\n\nsummary(glm_cancer_pov)\n\n\nCall:\nglm(formula = number_death ~ poverty, family = poisson, data = cancer_reg)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  5.1946575  0.0114207  454.85   &lt;2e-16 ***\npoverty     -0.0243758  0.0006472  -37.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 74152  on 526  degrees of freedom\nResidual deviance: 72668  on 525  degrees of freedom\nAIC: 75821\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe model results show a significant association between the number of cancer deaths and rates of poverty in counties. The coefficient estimates from the model output can be used to construct the linear equation fit to this data:\n\n\\(ln\\)(number of deaths) = 5.19 -0.02 \\(\\times\\) poverty\n\nAs the coefficient estimates are related to the natural log of the outcome. However, we are not able to interpret these effects on this scale, we must back-transform the outcome using the exponential function. If we apply the exponential function to the left-hand side of this equation, we must do the same to the right side to ensure equality. Therefore, the model equation becomes:\nnumber of deaths = exp(5.19 -0.02 \\(\\times\\) poverty)\n= exp(5.19) \\(\\times\\) exp(-0.02 \\(\\times\\) poverty)\n= 180.31 \\(\\times\\) 0.98 \\(^{poverty}\\)\nThe transformed intercept value, 180.31, is the expected number of cancer deaths where no one in the county lived in poverty.\nThe coefficient associated with poverty level, 0.98, now describes the multiplicative relationship between poverty and cancer deaths. For every 1 percentage point increase in poverty, the number of deaths is expected to decrease (as the transformed coefficient is below 1, no difference). This decrease can be converted into the percentage change to make it easier to communicate:\nFirst, we find the difference between the multiplicative change and no difference (1 in this case): 1 - 0.9759 = 0.0241.\nThis represents the proportion change in the outcome. To convert a proportion into a percentage, we simply multiply it by 100%:\n0.0241 \\(\\times\\) 100% = 2.41 %.\nFor every 1 percentage point increase in poverty, the number of deaths is expected to decrease by 2.41 %. To find the expected difference in counties where the poverty level was 10% higher, we multiple the transformed coefficient by itself 10 times and convert it into a percentage change:\n0.9759 \\(^{10}\\) = 0.7837\n1 - 0.7837 = 0.2163\n0.2163 \\(\\times\\) 100% = 21.63 %.\nThe confidence intervals for coefficient estimates can be obtained using the confint function we used before. However, the interval will be presented on the transformed scale and so we need to apply the exponential function before interpreting the results:\n\nround(exp(confint(glm_cancer_pov)), 2)\n\n             2.5 % 97.5 %\n(Intercept) 176.31 184.39\npoverty       0.97   0.98\n\n\nTherefore, we are 95% confident that the relative number of deaths in a county with a poverty level 1 percentage point higher will be between 0.97 and 0.98.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "04_glm.html#poisson-regression-for-rates",
    "href": "04_glm.html#poisson-regression-for-rates",
    "title": "4  Generalised linear models",
    "section": "4.3 Poisson regression for rates",
    "text": "4.3 Poisson regression for rates\nThe previous model showed there was a negative relationship between the number of deaths from cancer in the US and poverty. This is the opposite of what we would expect to see and could be an indication that there are confounding factors. As with linear regression, we must consider whether there are background factors that may be distorting the results of our model that must be included to obtain valid results.\nThe counties in our sample vary greatly in size:\n\n\n\n\n\n\n\n\n\nUsing the number of deaths as the outcome does not account for the differences in county populations. The higher the population, the more people that are at risk. Failure to adjust for this means we may just be measuring the relationship between poverty and population.\nTo avoid masking the relationship we are interested in modelling, we can introduce the population into the model. This can be done by modelling the rate of deaths rather than the raw count. The population is then added into the model as an offset term.\nThe offset is the inverse of the terms we would multiply the count by to convert it to the rate we want to model. It is often easier to write out the rate by hand and then work backwards to understand the offset term that will be needed:\nmortality rate per 10,000 people = \\(\\frac{number of deaths}{population} \\times\\) 10,000\nThe count would need to be multiplied by \\(\\frac{10,000}{population}\\). The offset is the inverse of this which is  1 \\(\\div \\frac{10,000}{population}\\) = \\(\\frac{population}{10,000}\\). As the outcome is modelled on the natural log scale, this offset will also need to be transformed. This will be added to the glm function when fitting a model using the offset argument:\n\ncancer_reg &lt;- mutate(cancer_reg, \n                     offset_rate = log(population_2015 / 10000))\n\nglm_mort_rate &lt;- glm(number_death ~ poverty + offset(offset_rate), \n                     data = cancer_reg, family = \"poisson\")\n\nsummary(glm_mort_rate)\n\n\nCall:\nglm(formula = number_death ~ poverty + offset(offset_rate), family = \"poisson\", \n    data = cancer_reg)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 2.8256966  0.0125010  226.04   &lt;2e-16 ***\npoverty     0.0105205  0.0007165   14.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 4101.4  on 526  degrees of freedom\nResidual deviance: 3888.2  on 525  degrees of freedom\nAIC: 7041.7\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe offset term does not have a coefficient estimated by the model as it is considered part of the outcome.\n\n\nThe new model produces the following equation:\n\\(ln\\)(number of deaths) = 2.83 + 0.01 \\(\\times\\) poverty + \\(ln \\left( \\frac{10,000}{population} \\right)\\)\nTo ensure the coefficient estimates are related to the outcome of interest, the mortality rate per 10,000 people, the offset must be moved to the left side of the equation. This is done by subtracting the offset from both sides:\n\\(ln\\)(number of deaths) - \\(ln \\left( \\frac{10,000}{population} \\right)\\) = 2.83 + 0.01 \\(\\times\\) poverty\nUsing the natural log properties introduced earlier2, the left side of the equation becomes the natural log of the mortality rate per 10,000:\n\\(ln \\left( number of deaths \\div \\frac{10,000}{population} \\right)\\) = \\(ln\\) (mortality rate per 10,000) = 2.83 + 0.01 \\(\\times\\) poverty\nCoefficient estimates currently relate to the natural log of the mortality rate which we are not able to interpret. Therefore, we must apply the exponential function to each side of the equation to fix this:\nmortality rate = \\(exp\\)(2.83 + 0.01 \\(\\times\\) poverty) = \\(exp\\)(2.83) \\(\\times exp\\) (0.01) \\(^{poverty}\\) = 16.87 \\(\\times\\) 1.01\\(^{poverty}\\)\nAlthough this model uses the same data as the first, the model now shows a positive association between poverty levels and mortality rate (as poverty increases by a percentage point, the mortality rate is expected to increase 1.0106 - 1 \\(\\times\\) 100% = 1.06 %). This indicates that the previous model, using count as the outcome, was modelling differences in population rather than cancer deaths.\nThe 95% confidence interval for poisson models of rates is produced in the same way as those for counts. Remember to transform the interval using the exponential function before interpreting these values.\n\n1round(exp(confint(glm_mort_rate)), 2)\n\n\n1\n\nI have rounded the interval to ensure the output is tidier.\n\n\n\n\n            2.5 % 97.5 %\n(Intercept) 16.46  17.29\npoverty      1.01   1.01",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "04_glm.html#glm-model-diagnostics",
    "href": "04_glm.html#glm-model-diagnostics",
    "title": "4  Generalised linear models",
    "section": "4.4 GLM model diagnostics",
    "text": "4.4 GLM model diagnostics\nEach form of generalised linear model has a different set of assumptions that must be checked to ensure results are valid. The assumptions will depend on the link function that is applied to the outcome, and the distribution that the outcome is expected to come from (for linear models, this is the normal distribution, for poisson models it is poisson, etc.).\nFor example, GLMs have a linearity assumption, but that the transformed outcome can be described using a linear equation containing covariates in the model.\n\n4.4.1 Poisson regression assumptions\nAs with linear models, observations used to fit a poisson model must be independent of one another, and explanatory variables included in the model must not be dependent on one another. However, there are some important differences between assumptions made about the model response and residuals.\nOne of the main differences between the assumptions underpinning linear and poisson regression models is that the count (or rate) outcome is assumed to follow a poisson distribution. A key assumption of the poisson distribution is that the mean and variance are equal. Therefore, the assumption that residuals have a constant variance is not appropriate.\nRather than considering the raw residuals (the difference between the observed and expected outcomes) for poisson models, Pearson residuals and deviance residuals give more insight into model validity and fit.\n\n4.4.1.1 Pearson residuals\nPearson residuals (\\(r_i^p\\)) standardise raw residuals by dividing the difference between observed (\\(y_i\\)) and predicted (\\(\\hat{y}_i\\)) outcomes by the standard deviation. For poisson regression, if the model (and poisson distribution assumption) is valid, the standard deviation will be the square root of the mean, or predicted outcome of the model:\n\n\\(r_i^p = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\hat{y}_i}}\\)\n\nIf the outcome (and therefore the residuals) followed a poisson distribution, we would expect these standardised Pearson residuals to follow a normal distribution, with a constant variance, and a mean of 0.\nIn R, the Pearson residuals can be calculated and plotted using the residuals function and specifying the type argument:\n\npearson_resid &lt;- tibble(glm_mort_rate$model) %&gt;% \n  mutate(residuals = residuals(glm_mort_rate, type = \"pearson\"),\n         id = row_number(.))\n\nggplot(data = pearson_resid) + \n  geom_point(aes(y = residuals, x = id)) +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\nggplot(data = pearson_resid) +\n  geom_histogram(aes(x = residuals), colour = \"black\", fill = \"grey45\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nBoth plots show a ‘funnel’ shape, showing that the poisson assumption is not valid for this data. We can refer to the data to try and examine why these observations are not as well represented by the model as other points.\n\n\n4.4.1.2 Deviance residuals\nThe deviance was introduced in Section 3.3.2 as a model comparison tool. Deviance quantifies how much the current model deviates from a hypothetical (and totally useless) full model. The lower the deviance, the better the model fits the sample data. The deviance alone is not particularly useful as the full model overfits the sample and is not able to provide inferences to the target population it is drawn from. However, it can be combined with other information to give insights about the model fit.\nDeviance residuals are estimated by multiplying the square root of the deviance contribution, \\(d_i\\) of an observation by\n\n+1 if the observed count is higher than the predicted count,\n0 if the observed and predicted counts are equal, or\n-1 if the observed count is lower than the predicted count.\n\nFor poisson regression, the deviance contribution for observation \\(i\\) is:\n\n\\(d_i = 2 \\left[ y_i log \\left(\\frac{y_i}{\\hat{y}_i} \\right) - (y_i - \\hat{y}_i) \\right]\\)\n\nLarge deviance residuals indicate that the model is not fitting an observation well. Plotting these deviance residuals can help identify potential outliers, influential values, or could help improve a model by highlighting a group that is not represented by the current model. In R, the deviance residuals are calculated (and plotted) as follows:\n\ndev_resid &lt;- tibble(glm_mort_rate$model) %&gt;% \n  mutate(residuals = residuals(glm_mort_rate, type = \"deviance\"),\n         id = row_number(.))\n\nggplot(data = dev_resid) + \n  geom_point(aes(y = residuals, x = id)) +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\nggplot(data = dev_resid) +\n  geom_histogram(aes(x = residuals), colour = \"black\", fill = \"grey45\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nDeviance residuals produce similar results as Pearson residuals when using poisson regression. However, they provide a more generalised model check for regression types other than poisson.\n\n\n\n\n\n\nNote\n\n\n\nIn reality, we would not use both the Pearson and deviance residuals when checking a regression model. The choice of residual depends on the intention of the check and whether we are communicating the results. The deviance residuals are presented here to give a generalised tool that can be applied to other GLMs.\n\n\n\n\n\n4.4.2 Equidispersion\nBoth the Pearson and deviance residual plots showed non-equal variance of residuals. This could be an indication of overdispersion. Overdispersion occurs when the poisson assumption that the mean and variance of the outcome are equal is not valid. When this is the case, other more flexible models may be required.\nTo test for overdispersion, we can use the dispersiontest function which is part of the AER package in R. This function tests the hypothesis that the outcome mean (\\(\\mu\\)) and variance (\\(var(y)\\)) are equal, against an alternative that the variance takes the form:\n\n\\(var(y) = (1 + \\alpha) \\times \\mu = dispersion \\times \\mu\\)\n\nThe output provides an estimate of the dispersion parameter (which would take the value 0 if the mean and variance are equal) and a p-value testing the null hypothesis of equidispersion.\n\nlibrary(AER)\n\ndispersiontest(glm_mort_rate, trafo = 1)\n\n\n    Overdispersion test\n\ndata:  glm_mort_rate\nz = 7.6555, p-value = 9.627e-15\nalternative hypothesis: true alpha is greater than 0\nsample estimates:\n   alpha \n6.235632 \n\n\nThe results show clear overdispersion. On average, the variance is 7.24 times higher than the mean. The p-value is too small to be printed in its entirety, indicating that this overdispersion is statistically significant.\nA poisson model would not be appropriate in this case. Other models, such as a quasipoisson or negative binomial model, which allow for unequal mean and variance, may be more appropriate.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "04_glm.html#exercise-3",
    "href": "04_glm.html#exercise-3",
    "title": "4  Generalised linear models",
    "section": "Exercise 3",
    "text": "Exercise 3\nUsing the data, fit an appropriate model to answer the research question:\nIs cancer mortality associated with poverty levels in the USA?\nEnsure that the mode contains any variables you consider necessary, and check that it is valid before using it to answer the research question.\n\n\n\n\n\n\nExercise hint\n\n\n\n\n\nAs with previous questions, plot the outcome and variables you believe may be important covariates to generate hypotheses about best fitting models.\nUse model comparison techniques such as information criterions and prediction errors to identify the most parsimonious model, ensuring that all variables that need to be in the model are present.\nTest model assumption, including checking for multicollinearity using vif, plotting Pearson residuals, and checking for equidispersion.\nIf the model is still overdispersed, try using family = quasipoisson instead, which assumes the variance is proportional to the mean, rather than equal. The summary of this model will include a dispersion parameter estimate, but model output is interpreted in the same way as poisson regression.\nIf you are REALLY stuck, an example solution can be found here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "04_glm.html#footnotes",
    "href": "04_glm.html#footnotes",
    "title": "4  Generalised linear models",
    "section": "",
    "text": "simple = only one covariate↩︎\n\\(ln(a) - ln(b) = ln(a \\div b)\\)↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalised linear models</span>"
    ]
  },
  {
    "objectID": "05_discussion.html",
    "href": "05_discussion.html",
    "title": "5  Discussion",
    "section": "",
    "text": "Generalised linear models are powerful statistical tools that can be applied to a wide range of data and situations. The choice of the most appropriate model to address a research question will depend on the type of outcome, but also:\n\nThe intention of the model: which variables must be included to answer the research question?\nCommon sense and background knowledge: what do we know about the context of the data and what are the known confounders?\nParsimony: which model provides the simplest solution to our problem without losing any information?\n\nDo not choose a regression model solely based on p-values!!\nAll models must be checked to ensure that any assumptions are met and the results are valid. All GLMs require observations to be independent of one another. This means that there is no clustering, repeated measures, or autocorrelation within the data. Where this assumption is not valid, multilevel models (also known as mixed effect, random effect, GLMMs, or hierarchical models) should be considered.\nGLMs assume that the relationships between covariates and the (link-transformed) outcome are linear. Where this is not the case, covariates can be transformed before they are included into the model, for example polynomial regression. Where the relationship is more complex or unknown, consider generalised additive models (GAMs), which are able to for non-linear data.\nFinally, note that the models shown in these notes and exercise solutions are not definitive. Choice of model is often subjective and context-specific.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "data_description.html",
    "href": "data_description.html",
    "title": "Data description",
    "section": "",
    "text": "Where is this data from?",
    "crumbs": [
      "Appendices",
      "Data description"
    ]
  },
  {
    "objectID": "data_description.html#description-of-variables",
    "href": "data_description.html#description-of-variables",
    "title": "Data description",
    "section": "Description of variables",
    "text": "Description of variables",
    "crumbs": [
      "Appendices",
      "Data description"
    ]
  },
  {
    "objectID": "exercise_solutions.html",
    "href": "exercise_solutions.html",
    "title": "Appendix A — Exercise solutions",
    "section": "",
    "text": "A.1 Exercise 1\nUsing appropriate visualisations, investigate whether there are other variables that may explain differences in body mass. Consider whether any of these variables may be confounding the relationship between body mass and flipper length, and whether they should be included in the model.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Exercise solutions</span>"
    ]
  },
  {
    "objectID": "exercise_solutions.html#exercise-1",
    "href": "exercise_solutions.html#exercise-1",
    "title": "Appendix A — Exercise solutions",
    "section": "",
    "text": "Solution\nBody mass and flipper length are both likely to differ between penguin species. Changing the colour of points for each species will allow us to visualise these differences:\n\nggplot(data = penguins) +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  labs(x = \"flipper length (mm)\", y = \"body mass (g)\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nSpecies is clearly strongly associated with both body mass and flipper length, although the gradient of these associations appear similar across species.\nThis scatterplot could be extended to investigate whether these trends differ between sexes. Adding an additional variable to the previous scatterplot may overload it, making the relationships difficult to interpret. Instead, we could facet the graphs, showing a scatterplot per sex on the same graph area, with the same axes:\n\nggplot(data = na.omit(penguins)) +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  labs(x = \"flipper length (mm)\", y = \"body mass (g)\") +\n  facet_wrap(vars(sex), ncol = 2) +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nHere, it appears that the male pengions are larger on average than the females. The relationships between body mass, flipper length and species appear equal between sexes.\nThe data also contains information about penguins’ bill length and depth which may also by a predictor of body mass. These can be plotted against body mass in a scatterplot, replacing flipper length, or could be included into the original scatterplot by using a continuous colour scale.\n\nggplot(data = penguins) +\n  geom_point(aes(x = bill_length_mm, y = body_mass_g)) +\n  labs(x = \"bill length (mm)\", y = \"body mass (g)\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\nggplot(data = penguins) +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, \n                 colour = bill_length_mm)) +\n  scale_colour_viridis_c(name = \"bill length (mm)\") +\n  labs(x = \"flipper length (mm)\", y = \"body mass (g)\") +\n  theme_light(base_size = 12)\n\n\n\n\n\n\n\n\nThere appears to be a positive association between bill length and body mass, but it is not as strong as the one between flipper length and body mass.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Exercise solutions</span>"
    ]
  },
  {
    "objectID": "exercise_solutions.html#exercise-2",
    "href": "exercise_solutions.html#exercise-2",
    "title": "Appendix A — Exercise solutions",
    "section": "A.2 Exercise 2",
    "text": "A.2 Exercise 2\nUsing everything you have learned up to this point, use linear regression to answer the research question posed earlier:\nIs body mass of penguins in the Palmer Archipelago related to their flipper size?\n\nSolution\nFrom our research question, we know that our model must have body mass as the outcome and flipper length as an explanatory variable. Previous exploratory analysis showed that sex and bill length were also associated to body mass. We can add these variables into a linear model and consider whether it improves the model fit. We may also try removing species from the model as this appeared to lead to heteroskedasticity in the residuals:\n\n1lm_flipper &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\nlm_flipper_sex &lt;- lm(body_mass_g ~ flipper_length_mm + sex, \n2                     data = penguins)\n\nlm_flipper_bill &lt;- lm(body_mass_g ~ flipper_length_mm + \n3                        bill_length_mm, data = penguins)\n\nlm_full &lt;- lm(body_mass_g ~ flipper_length_mm + sex + \n4                bill_length_mm, data = penguins)\n\n\n1\n\nWe will begin with the simplest possible model for comparison, one containing just body mass and flipper length.\n\n2\n\nA model with sex instead of species which was found to be related to body mass.\n\n3\n\nAdd bill length to see if this improves the initial model.\n\n4\n\nA model with all potential exploratory variables (besides species).\n\n\n\n\nWe can compare these models in various ways, including the adjusted R-squared, information criterions, and prediction errors. Below is a table containing these comparisons for each model.\n\n\n\n\n\nmodel\nadjusted R-squared\nAIC\nRMSE\n\n\n\n\nflipper only\n0.7582837\n5062.855\n393.1236\n\n\nflipper + sex\n0.8046607\n4862.484\n354.2762\n\n\nflipper + bill length\n0.7585415\n5063.482\n392.3357\n\n\nflipper + bill length + sex\n0.8047466\n4863.327\n353.6612\n\n\n\n\n\nThe model containing flipper length and sex slightly outperformed the full momdel according to the adjusted R-squared and AIC, but had a slightly lower RMSE. As the bill length is not important to our research question and the model is not being used for prediction, we will choose the simplest possible model and remove bill length.\nBefore we use this model to answer our research question, we must ensure that the model is valid. Remember, the assumptions we need to check are Linearity, Independent covariates, Normally distributed residuals, with Equal variance.\n\nvif(lm_flipper_sex) \n\nflipper_length_mm               sex \n         1.069646          1.069646 \n\n\nAll VIFs are very low, indicating no issues with multicollinearity.\n\nlm_flipper_sex_resid &lt;- lm_flipper_sex$model %&gt;%  \n  mutate(residuals = residuals(lm_full))\n\nggplot(data = lm_flipper_sex_resid) +\n  geom_histogram(aes(x = residuals), colour = \"black\", fill = \"grey45\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_light(base_size = 12) \n\n\n3\n\nPlot the residuals against each covariate to check the linearity assumption.\n\n\n\n\n\n\n\n\n\n\nggplot(data = lm_flipper_sex_resid) +\n  geom_point(aes(x = body_mass_g, y = residuals)) +\n  geom_hline(yintercept = 0, colour = \"darkorange3\") +\n  labs(x = \"body mass (g)\", y = \"residuals\") +\n  theme_light(base_size = 12) \n\n\n\n\n\n\n\n3ggplot(data = lm_flipper_sex_resid) +\n  geom_point(aes(x = flipper_length_mm, y = residuals)) +\n  geom_hline(yintercept = 0, colour = \"darkorange3\") +\n  labs(x = \"flipper length (mm)\", y = \"residuals\") +\n  theme_light(base_size = 12) \n\n\n\n\n\n\n\nggplot(data = lm_flipper_sex_resid) + \n  geom_point(aes(x = sex, y = residuals)) +\n  geom_hline(yintercept = 0, colour = \"darkorange3\") +\n  labs(x = \"sex\", y = \"residuals\") +\n  theme_light(base_size = 12)  \n\n\n\n\n\n\n\n\nThe residuals are approximately normal, their variance is approximately constant, and there is no evidence to suggest that the linearity assumption would not be valid. Therefore, we can use this model to answer our research question.\n\nsummary(lm_flipper_sex)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + sex, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-910.28 -243.89   -2.94  238.85 1067.73 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5410.300    285.798 -18.931  &lt; 2e-16 ***\nflipper_length_mm    46.982      1.441  32.598  &lt; 2e-16 ***\nsexmale             347.850     40.342   8.623 2.78e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 355.9 on 330 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.8058,    Adjusted R-squared:  0.8047 \nF-statistic: 684.8 on 2 and 330 DF,  p-value: &lt; 2.2e-16\n\nconfint(lm_flipper_sex)\n\n                        2.5 %      97.5 %\n(Intercept)       -5972.51535 -4848.08510\nflipper_length_mm    44.14697    49.81738\nsexmale             268.49120   427.20930\n\n\nBased on these results, we can infer that there is a significantly positive association between flipper length and body mass of the Palmer penguins. On average, body mass is expected to increase by 46.98g for every 1mm increase in flipper length, We are 95% confident that this increase is between 44.15g and 49.82g in the target population.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Exercise solutions</span>"
    ]
  },
  {
    "objectID": "exercise_solutions.html#exercise-3",
    "href": "exercise_solutions.html#exercise-3",
    "title": "Appendix A — Exercise solutions",
    "section": "A.3 Exercise 3",
    "text": "A.3 Exercise 3\nUsing the data, fit an appropriate model to answer the research question:\nIs cancer mortality associated with poverty levels in the USA?\nEnsure that the mode contains any variables you consider necessary, and check that it is valid before using it to answer the research question.\n\nSolution\nThe model we require will have cancer mortality (number of deaths with population as an offset) as an outcome and must contain a measure of poverty to answer the research question. Other variables from the data that may be important include the average age of a county, their access to medicare, and possibly income (although this will likely be highly correlated to poverty).\nFirst, we should explore the data and plot these variables to understand their bivariate relationships. Rather than do this manually, we could use the ggpair function from the GGally package:\n\nlibrary(GGally)\n\ncancer_reg_clean &lt;- cancer_reg %&gt;% \n  mutate(mortality_rate = (number_death / population_2015) * 10^5,\n         offset_rate = log(population_2015 / 10^5)) %&gt;% \n  select(mortality_rate, number_death, population_2015, age, income, poverty,\n         medicare, offset_rate)\n\ncancer_reg_clean %&gt;% \n  select(-number_death, -population_2015, -offset_rate) %&gt;% \n  ggpairs()\n\n\n\n\n\n\n\n\nAs expected, both income and medicare access are highly correlated to poverty. Although this does not necessarily make them dependent on each other, the interpretation of mode coefficients can be complicated by their inclusion. To remove this issue, we can add age to the original model to see if it improves the fit.\n\npois_pov &lt;- glm(number_death ~ poverty + offset(offset_rate), \n                data = cancer_reg_clean, family = poisson)\n\npois_pov_age &lt;- glm(number_death ~ poverty + age + offset(offset_rate), \n                    data = cancer_reg_clean, family = poisson)\n\nThe adjusted R-squared measure is only appropriate for linear models. However, we can still use information criterions and prediction errors to compare models:\n\n\n\n\n\nmodel\nAIC\nRMSE\n\n\n\n\npoverty only\n7041.657\n192.3616\n\n\npoverty + age\n4330.177\n192.3497\n\n\n\n\n\nAdding age appears to vastly improve the AIC but only slightly improve prediction according to the RMSE. The poisson model containing just poverty showed strong evidence of overdispersion, therefore we must check this model to find whether the addition of age has removed the issue:\n\ndispersiontest(pois_pov_age, trafo = 1)\n\n\n    Overdispersion test\n\ndata:  pois_pov_age\nz = 4.0437, p-value = 2.63e-05\nalternative hypothesis: true alpha is greater than 0\nsample estimates:\n   alpha \n1.141831 \n\n\nAlthough the dispersion parameter is lower than the poverty only model, there is still evidence of overdispersion. Therefore, a quasipoisson model may be more appropriate:\n\nquasi_pov_age &lt;- glm(number_death ~ poverty + age  + offset(offset_rate), \n                     data = cancer_reg_clean, family = quasipoisson)\n\nsummary(quasi_pov_age)\n\n\nCall:\nglm(formula = number_death ~ poverty + age + offset(offset_rate), \n    family = quasipoisson, data = cancer_reg_clean)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.134120   0.058920   53.19   &lt;2e-16 ***\npoverty     0.018326   0.001076   17.02   &lt;2e-16 ***\nage         0.047598   0.001326   35.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 2.175137)\n\n    Null deviance: 4101.4  on 526  degrees of freedom\nResidual deviance: 1174.7  on 524  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe summary of quasipoisson models contains an estimate of the dispersion parameter. Quasipoisson and Poisson models are equivalent when the dispersion parameter is 1. As the parameter was estimated above that, this is a clear indication the poisson model was not appropriate for this data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Exercise solutions</span>"
    ]
  }
]